{"cells":[{"cell_type":"markdown","metadata":{"id":"3fkC_EI6Lok1"},"source":["# Web Scraping"]},{"cell_type":"markdown","metadata":{"id":"DFf05_ngLok3"},"source":["<div class=\"alert alert-info\">\n","Web scrapers are programs that read and extract data from websites.\n","</div>\n","\n","The websites generally consist of text and HTML tags. Those tags define the structure and layout of the document. For instance, we see tags like `<i>` to italicize text, tags like `<table>` to define tables of data. We're going to extract a table from a Wikipedia page in this example.  \n","\n","The tool we'll use to find and extract the table is a Python library called BeautifulSoup.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssplwL56Lok4","metadata":{}},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","\n","# we'll import pandas since we'll need it later\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"qIh4z9OnLok5"},"source":["### Getting the Web Content\n","The first thing to do is to grab the contents of our website.   \n","Make a variable called `skycrapercenter_url` that holds the link to the tallest buildings page on Wikipedia. That URL is:  \n","[https://www.skyscrapercenter.com/buildings](https://www.skyscrapercenter.com/buildings)  \n","\n","Use `requests.get()` to open the URL using `skycrapercenter_url` as its input. That will give you the webpage: Store this in a variable named `skyscraper_page`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaPwSUWRLok5","metadata":{}},"outputs":[],"source":["# site_url will hold the URL for the Wikipedia building page\n","skycrapercenter_url = \"https://www.skyscrapercenter.com/buildings\"\n","\n","# get the web page using the URL request\n","skyscraper_page = requests.get(skycrapercenter_url)"]},{"cell_type":"markdown","metadata":{"id":"MTCX7dLrLok5"},"source":["Let's print the first 1000 characters of the skyscrapercenter.com entry. We do that by requesting the `.content` of `skyscraper_page`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOAJ1QHoLok6","metadata":{}},"outputs":[],"source":["# print the first 1000 characters of the web page\n","print(skyscraper_page.content[0:1000])"]},{"cell_type":"markdown","metadata":{"id":"zF1QRND9Lok6"},"source":["You can see that the page mentions \"html\" in the first few words. Otherwise, there's a lot of text that doesn't appear to have much meaning. It's certainly not obvious how we'd find our data table in all of this HTML text.  \n","\n","But the HTML tags are the key to finding the table that we'd like. We can use BeautifulSoup to examine a complex document like this and look for the information we're interested in."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flUkROUALok7","metadata":{}},"outputs":[],"source":["# Read the webpage with BeautifulSoups HTML parser\n","soup_page = BeautifulSoup(skyscraper_page.content, 'html.parser')"]},{"cell_type":"markdown","metadata":{"id":"kGxCZYryLok7"},"source":["BeautifulSoup has some simple commands to find features within a web site. For example, we can extract a web page's title."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z259nNxaLok7","metadata":{}},"outputs":[],"source":["# find and print the web page title\n","print(\"Title: \")\n","print(soup_page.title)"]},{"cell_type":"markdown","metadata":{"id":"kAtkZYHILok7"},"source":["Note that the text between the `<title>...</title>` tags is the title of the page. If we just want the text and not the surroundings tags, we can do this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4s0qI4TGLok8","metadata":{}},"outputs":[],"source":["print(\"Title: \")\n","print(soup_page.title.string)"]},{"cell_type":"markdown","metadata":{"id":"HPVhvFQELok8"},"source":["Notice we get the title, but its's surrounded by a **lot** of space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU4JH0TELok8","metadata":{}},"outputs":[],"source":["title_string = ' '.join(soup_page.title.string.split())\n","title_string"]},{"cell_type":"markdown","metadata":{"id":"_E47NywCLok8"},"source":["We can use the function `find` on a BeautifulSoup page to find different tags. For example, let's find the first tag in the document that uses the paragraph, or `<p>`, tag:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUvTgODXLok9","metadata":{}},"outputs":[],"source":["print(\"First <p> tag: \")\n","print(soup_page.find('p'))"]},{"cell_type":"markdown","metadata":{"id":"dUyOOTGPLok9"},"source":["There's a lot in that paragraph! You can pull out other tags in the document in similar ways.  \n","\n","Remember that we identified the HTML table in the Wikipedia article back in Canvas. That table starts with the following text:\n","\n","```\n","<table id=\"buildingsTable\" class=\"custom-table buildings-table bg-white pt-1\">\n","```\n","\n","What is all that stuff after `table`? It turns out we don't really care. All that matters is that it helps us differentiate this table from any others that might be in the same document. It helps us know this is the right thing to extract. So let's do that using `find`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-pIVk20Lok9","metadata":{}},"outputs":[],"source":["data_table = soup_page.find('table', id=\"buildingsTable\")\n","\n","# Let's make sure we found the right table\n","print(data_table)"]},{"cell_type":"markdown","metadata":{"id":"Bwllyfp0Lok9"},"source":["There's a lot there, but you should see the column titles in the first few lines of the table HTML.  \n","\n","The next part is tricky. It involves the same idea: Using `find` to look through the table and extract the data elements. In the Canvas session we said we care about the name of the building, its height, the number of floors, the city and country, and the year it was built. We have to look through the table rows (`tr`) and pull the table data (`td`) in each column.  \n","\n","Don't worry about how this works yet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bt0kmVcYLok9","metadata":{}},"outputs":[],"source":["#\n","building_names = []\n","heights = []\n","heights_in_feet = []\n","number_of_floors = []\n","cities = []\n","countries = []\n","completion_dates = []\n","\n","# for every row (<tr>)\n","for row in data_table.findAll('tr'):\n","\n","    # get all of the table data (<td>) in each column\n","    cells = row.findAll('td')\n","\n","    # Skip rows that aren't 9 columns long\n","    if len(cells) != 9:\n","        continue\n","\n","    try:\n","        building_names.append(cells[1].find('a').text.strip())\n","        cities.append(cells[2].find('a').text.strip())\n","        completion_dates.append(int(cells[4].find('p').text.strip()))\n","        heights.append(cells[5].find('p').text.strip())\n","        number_of_floors.append(int(cells[6].find('p').text.strip()))\n","\n","    except:\n","        break\n"]},{"cell_type":"markdown","metadata":{"id":"WYsjz1nZLok-"},"source":["### Putting the data into a dataframe\n","\n","Our netx step is to take what we read from the skyscrapercenter.com page and convert it into a Pandas dataframe.\n","\n","Let's make a Python dictionary with each of the lists we created above.. These will have roughly the same titles as the table on the skyscraper page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gyb0J8e8Lok-","metadata":{}},"outputs":[],"source":["data = {'name': building_names,\n","        'city': cities,\n","        'year': completion_dates,\n","        'height': heights,\n","        'floors': number_of_floors}"]},{"cell_type":"markdown","metadata":{"id":"oHiHsG3NLok-"},"source":["Now we can combine the `column_names` and the `data` into a Pandas dataframe. Let's put the dataframe into a variable named `df`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIfD88kCLok-","metadata":{}},"outputs":[],"source":["df = pd.DataFrame(data)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"qJ7S8RHYLok-"},"source":["And there you have it, a Pandas dataframe with all of the data from the skyscraper webpage! That was a lot of work, so let's save the dataframe so we don't need to scrape the web page again. We use `to_csv` to convert the dataframe into a CSV file. We also add the argument `index=False`; this will drop the index values (0,1,2...) from the CSV file. They'll return when we reload the file into a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["df.to_csv(\"tallest_skyscrapers.csv\", index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cgms","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
