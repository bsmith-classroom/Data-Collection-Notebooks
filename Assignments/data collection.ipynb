{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "In this assignment, ...\n",
    "\n",
    "We'll start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "You will see some lines of code that call the `assert` function. **DO NOT** change or update or delete the assert statements. `assert` tests to make sure your code is running properly. These statements can help you see if things are working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping\n",
    "Boston College plays home football games in its Alumni Stadium. Many BC fans think think this is a big stadium as it can seat 44,500 people. That's a lot of people, but it's nowhere near the largest in the world. You may know that Michigan Stadium, also known as \"The Big House\", is tha largest stadium in the United States with a capacty of 107,601. Big, indeed.  \n",
    "\n",
    "Like so many things, Wikipedia has a [page](https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity) dedicated to the largest stadiums in the world. You'll see some interesting things on that page. In fact, the data may be counter to your initial expectations.\n",
    "\n",
    "1. The largest stadium in the world is Narenda Modi, a cricket ground in India. Cricket is enormously popular in countries that don't start with \"USA,\" so this might not be a surprise.\n",
    "\n",
    "2. Number 2 is Rungrado 1st of May Stadium in North Korea. The North Korean national football (American translation: soccer) team plays in this stadium. Football makes sense because of its even more enormous popularity around the globe. North Korea...that one you may not have guessed.\n",
    "\n",
    "3. Numbers 3-10 are all in the United States, and their tenants are...college football teams. Not NFL teams--*college* football teams. These stadiums are so big that they may have have a larger capacooty than the population of the college town they sit in.\n",
    "\n",
    "And that's a question we might want to explore. What is the ratio of a stadium's capacity to the population of its city? To study that, we need to turn this Wikipedia page into data we can analyze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "Let's start by grabbing the contents of the Wikipedia page. \n",
    "  \n",
    "1. Start my making a new variable called `wikipedia_URL`. Assign the following URL to that variable:     \n",
    "[https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity](https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity). Make sure the URL is a string (in quotes).   \n",
    "\n",
    "2. Use the Python function `requests.get()` to get the contents of the page. Use `wikipedia_URL` as the function's input value. Assign this to variable called `wikipedia_page`.\n",
    "3. Make a variable named `soup_page`. Remember that the web scraping tool is called `BeautifulSoup`. You'll need to call `BeautifulSoup` with some arguments to pull the content from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "wikipedia_URL = \"https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity\"\n",
    "wikipedia_page = requests.get(wikipedia_URL)\n",
    "soup_page = BeautifulSoup(wikipedia_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON\"T CHANGE THIS CELL...Doing some simple asserts to make sure your code is working.\n",
    "assert wikipedia_URL\n",
    "assert wikipedia_page\n",
    "assert soup_page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we Have the Right Content?\n",
    "\n",
    "Use a Python function to get the title from the soup_page. Store it in a variable named `title_page`. Unfortunately, BeautifulSoup will return the title as \"tag\" type. You'll want to convert the value to a string before storing it in `title_page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "title_page = str(soup_page.title)\n",
    "print(title_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert title_page == \"<title>List of stadiums by capacity - Wikipedia</title>\"\n",
    "assert type(title_page) == str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the First Table\n",
    "Take a look at the [web page](https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity). You'll see **multiple** tables with stadum information. This would be a lot easier if all the data were in one table, but we don't have that luxury. Let's focus on finding the first table, the one with stadiums with Capacity of 100,000 or more.  \n",
    "\n",
    "You should call the function `fund` on the `soup_page` variable. Store the result in a variable named `over_100000_table`. \n",
    "\n",
    "What is it we want to find? It's a table with an HTML `class_` of `sortable wikitable`. Use those two values as the arguments to `find`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_100000_table = soup_page.find('table', class_='sortable wikitable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some asserts to make sure the code is working as expected.\n",
    "assert over_100000_table\n",
    "assert over_100000_table.name == 'table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table to see what we have\n",
    "print(over_100000_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print `over_100000_table`, you see HTML code defining a table. You can see the headers, the title of each column (e.g., \"Country\") and the contents of each row. Those data values in the rows are what we want to collect into a pandas dataframe.  \n",
    "\n",
    "But wait. We only got the *first* table containing stadiums with capacities >= 100,000. What if there's a way to grab **all** of the tables? We used BeautifulSoup's `find` method to get one table. Maybe there's a `findAll` method for this? Let's give it a try.  \n",
    "\n",
    "Make a variable named all_tables and use BeautifulSoup and `findAll()` to see what happens. Hint: The arguments will be the same as you used for `find()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = soup_page.find_all('table', class_='sortable wikitable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_tables\n",
    "\n",
    "# There are seven tables on the Wikipedia page, check to see we have 7\n",
    "assert len(all_tables) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`findAll()` returns a list of all the HTML elements you specify. In this case, there are seven tables on the Wikipedia page: `findAll` should return a list with seven table element to the `all_tables` variable.\n",
    "\n",
    "It's great to have the tables. Now we have to pull the data out of them. Let's try doing this slowly with a single table, the one stored in `over_100000_table`. Here's a simple way to understsnd what we need to do.\n",
    "\n",
    "```\n",
    "For every row in the table:\n",
    "    Let row_data = All of the data entries ('td') in the row\n",
    "    Use array indexing to store items in row_data into lists\n",
    "```\n",
    "\n",
    "This is a little tricky, so the code is provided. First, a simple example where we juse pick up the stadium names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list called stadium_names\n",
    "stadium_names = []\n",
    "\n",
    "# for every row (<tr>) in the HTML table\n",
    "for row in over_100000_table.findAll('tr'):\n",
    "    \n",
    "    # find all table data (<td>) in the row\n",
    "    row_data = row.findAll('td')\n",
    "\n",
    "    # The try statement will run code unless there's an error\n",
    "    # If there is, code execution jumps to the except statement\n",
    "    try:\n",
    "        stadium_name = row_data[0]\n",
    "        stadium_names.append(stadium_name)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# return stadium_names ot see what we collected\n",
    "stadium_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what we have in `stadium_names`: It's a list containing all of the data items in the first column of the table...perfect! Except it's not really what we want. We *really* want the stadium names as text. Instead, we have HTML code that contains the name of the stadium and a URL pointing to the stadium's wikipedia page. \n",
    "\n",
    "Luckily, BeautifulSoup provides a way to extract just the `text` from an HTML anchor (i.e., \\<a\\>). You can use `find()` to look for the \\<a\\> reference and then extract its text. Something like this:\n",
    "\n",
    "```\n",
    "data.find('a').text\n",
    "```\n",
    "\n",
    "But we can also just ask for the text directly like this:\n",
    "\n",
    "```\n",
    "data.text\n",
    "```\n",
    "\n",
    "Below is the same code we used above. See if you can add the `text` call where we append the first entry of `row_data` to `stadium_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list called stadium_names\n",
    "stadium_names = []\n",
    "\n",
    "# for every row (<tr>) in the HTML table\n",
    "for row in over_100000_table.findAll('tr'):\n",
    "    \n",
    "    # find all table data (<td>) in the row\n",
    "    row_data = row.findAll('td')\n",
    "\n",
    "    # The try statement will run code unless there's an error\n",
    "    # If there is, code execution jumps to the except statement\n",
    "    try:\n",
    "        ### HERE IS WHERE YOU WANT TO ADD THE CALL TO .text TO PULL OUT THE \n",
    "        ### DATA CELL'S TEXT\n",
    "        stadium_name = row_data[0].text\n",
    "        stadium_names.append(stadium_name)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# return stadium_names ot see what we collected\n",
    "stadium_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better. We now have a list of stadium names that we can add to a pandas dataframe. Here's how we'd do that. We'll create a variable named `stadium_df` and add the names to it. We'll also add a column name, `stadium_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list containing the column names for the dataframe\n",
    "column_names = ['stadium_name']\n",
    "\n",
    "# make a dataframe with the column name and stadium names\n",
    "stadium_df = pd.DataFrame(columns=column_names, data=stadium_names)\n",
    "\n",
    "stadium_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's a dataframe with stadium names. You might imagine that we can continue grabbing other data columns like the capacity, city, country, etc. These are all accessble by indexing the list named `row_data` in our code. Here's an example where we'll grab the `Region` column from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list called stadium_names\n",
    "stadium_names = []\n",
    "\n",
    "# also make an empty list called region_names\n",
    "region_names = []\n",
    "\n",
    "# for every row (<tr>) in the HTML table\n",
    "for row in over_100000_table.findAll('tr'):\n",
    "    \n",
    "    # find all table data (<td>) in the row\n",
    "    row_data = row.findAll('td')\n",
    "\n",
    "    # The try statement will run code unless there's an error\n",
    "    # If there is, code execution jumps to the except statement\n",
    "    try:\n",
    "        ### HERE IS WHERE YOU WANT TO ADD THE CALL TO FIND() TO PULL OUT THE \n",
    "        ### HYPERLINK/URL\n",
    "        stadium_name = row_data[0].text\n",
    "        stadium_names.append(stadium_name)\n",
    "\n",
    "        # Let's grab the region data. It's in the 5th column. But we use zero \n",
    "        # indexing, so it's actualy the 4th column\n",
    "        region_name = row_data[4].text\n",
    "        region_names.append(region_name)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# return region_names ot see what we collected\n",
    "region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the 11 largest stadium names and their regions. We can combine these into a dataframe. We did this earlier for the stadium names. This time, we'll create a new dataframe, called `stadium_info` with two columns, `stadium_name` and `region`.\n",
    "\n",
    "Here's the trick. The `DataFrame` constructor function essentiially wants you to pass data in row by row. We have two lists now that contain column data (`stadium_names` and `region_names`). What we really want is a list of lists that look something like this:\n",
    "\n",
    "```\n",
    "    [['Narendra Modi Stadium[1]', 'South Asia'],\n",
    "     ['Rungrado 1st of May Stadium', 'East Asia'],\n",
    "     ['Michigan Statium', 'North America'],\n",
    "    ...\n",
    "    ]\n",
    "\n",
    "```\n",
    "\n",
    "Notice that each one of those lists is a partial row from the original Wikipedia table. It's easy to make these  by hand if the lists are small. But it'd be time-consuming to do this for any sizeable amount of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's zip function\n",
    "Enter `zip`. `zip` takes multiple lists and returns an object containing new tuples that combine elements from each. The \"object\" part is a little confusing, so let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a list of letters\n",
    "letters = ['a', 'b', 'c']\n",
    "\n",
    "# And here's a list of numbers\n",
    "numbers = [1, 2, 3]\n",
    "\n",
    "# zip will return a zip object. Not quite what we want...yet.\n",
    "zip_object = zip(letters, numbers)\n",
    "print(zip_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A zip object isn't useful, but we can make it useful by simply forcing it to be a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_list = list(zip_object)\n",
    "print(zipped_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we want to see...a list of tuples that contain the first elements of the input lists, the second elements, etc.   \n",
    "\n",
    "Your turn. Make three lists containing:\n",
    "\n",
    "1) The names of three people you know. Store these in a variable named `people`.\n",
    "2) Your three favorite animals in a variable named `animals`.\n",
    "3) Three cities you've visited or would like to visit in a variable named 'cities`.\n",
    "\n",
    "Then use `zip` to make a new zip object. Calling `list` with the zip object should reveal a list with this structure:\n",
    "\n",
    "```\n",
    "    [[1st element of people, 1st element of animals, 1st element of cities],\n",
    "     [2nd element of people, 2nd element of animals, 2nd element of cities],\n",
    "     [3rd element of people, 3rd element of animals, 3rd element of cities]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "# three people you know\n",
    "people = ['Keith', 'Greg', 'Carl']\n",
    "\n",
    "# three animals\n",
    "animals = ['Cat', 'ELephant', 'Lemur']\n",
    "\n",
    "# three cities\n",
    "cities = ['Beijing', 'Rome', 'Leeds']\n",
    "\n",
    "# call zip below to combine the three lists you just made. \n",
    "zip_object = zip(people, animals, cities)\n",
    "\n",
    "# Use that zip object as the input to 'list' to reveal the \n",
    "# zipped content\n",
    "print(list(zip_object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check code with asserts\n",
    "assert people, \"You need to add people to the empty list\"\n",
    "assert animals, \"You need to add animals to the empty list\"\n",
    "assert cities, \"You need to add cities to the empty list\"\n",
    "assert isinstance(list(zip_object), list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using zip to create a pandas dataframe\n",
    "Now we can try this with real data. Fill in the code below to make a list of data rows in the `DataFrame` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list containing the column names for the dataframe\n",
    "column_names = ['stadium_names', 'region_names']\n",
    "\n",
    "# make a dataframe with the column name and stadium names\n",
    "# The data should be a zipped list\n",
    "stadium_df = pd.DataFrame(columns=column_names, data= list(zip(stadium_names, region_names))) #REPLACE WITH YOUR CODE)\n",
    "\n",
    "# We'll check some things here with assert statements\n",
    "assert stadium_df.shape == (11,2)\n",
    "\n",
    "stadium_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing all of the tables\n",
    "So far, we've extracted data fromn a single Wikipedia table. Let's go get the remaining tables so we can have a complete set of data.  \n",
    "\n",
    "Remember how we used BeautifulSoup to `find` a table with class id = `sortable wikitable`? Let's see what happens if we use `findAll`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = soup_page.findAll('table', class_='sortable wikitable')\n",
    "\n",
    "# print the length of all_tables\n",
    "print(f\"Beautiful Soup findAll returned {len(all_tables)} tables!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as hoped, `findAll` retrieved all of the data tables on the Wikipedia page. Now we can extract the data from each table exactly as we did earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list called stadium_names\n",
    "stadium_names = []\n",
    "\n",
    "# also make an empty list called region_names\n",
    "region_names = []\n",
    "\n",
    "# for every table in the list named all_tables, go get the data!\n",
    "for table in all_tables:\n",
    "\n",
    "    # for every row (<tr>) in the HTML table\n",
    "    for row in table.findAll('tr'):\n",
    "    \n",
    "        # find all table data (<td>) in the row\n",
    "        row_data = row.findAll('td')\n",
    "\n",
    "        # The try statement will run code unless there's an error\n",
    "        # If there is, code execution jumps to the except statement\n",
    "        try:\n",
    "            # Get the stadium name in the 0th column\n",
    "            stadium_name = row_data[0].text\n",
    "            stadium_names.append(stadium_name)\n",
    "\n",
    "            # Let's grab the region data. It's in the 5th column. \n",
    "            # But we use zero indexing, so it's actualy the 4th column\n",
    "            region_name = row_data[4].text\n",
    "            region_names.append(region_name)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"There are {len(stadium_names)} stadiums on the Wikipedia page!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the final pieces of the assignment. To the code below, please add:\n",
    "1. Variables named `capacities`, `city_names`, and `country_names`. \n",
    "2. In the Python `try:` statement, add the code needed to extract values from the Wikipedia table and insert those into the named variables.\n",
    "3. Create a new DataFrame called `stadium_info` that has the complete set of data with appropriate column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list called stadium_names\n",
    "stadium_names = []\n",
    "\n",
    "# also make an empty list called region_names\n",
    "region_names = []\n",
    "\n",
    "# YOUR CODE BELOW SHOULD CREATE NEW LISTS\n",
    "# capacities, city_names, country_names\n",
    "\n",
    "# for every table in the list named all_tables, go get the data!\n",
    "for table in all_tables:\n",
    "\n",
    "    # for every row (<tr>) in the HTML table\n",
    "    for row in table.findAll('tr'):\n",
    "    \n",
    "        # find all table data (<td>) in the row\n",
    "        row_data = row.findAll('td')\n",
    "\n",
    "        # The try statement will run code unless there's an error\n",
    "        # If there is, code execution jumps to the except statement\n",
    "        try:\n",
    "            # Get the stadium name in the 0th column\n",
    "            stadium_name = row_data[0].text\n",
    "            stadium_names.append(stadium_name)\n",
    "\n",
    "            ## INSERT YOUR CODE FOR CAPACITY, CITY, & COUNTRY BELOW\n",
    "\n",
    "\n",
    "            # Let's grab the region data. It's in the 5th column. \n",
    "            # But we use zero indexing, so it's \"really\" in the 4th column\n",
    "            region_name = row_data[4].text\n",
    "            region_names.append(region_name)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# make a list containing the column names for the dataframe\n",
    "# YOU FILL IN THE COLUMN NAMES\n",
    "column_names = []\n",
    "\n",
    "# make a dataframe with the column name and stadium names\n",
    "# The data should be a zipped list\n",
    "# YOUR CODE GOES IN THE DATAFRAME FUNCTION\n",
    "stadium_data = pd.DataFrame()\n",
    "\n",
    "print(f\"There are {stadium_data.shape[0]} rows, {stadium_data.shape[1]} columns in stadium_data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the last few entries in `stadium_data`. Compare these with the final entires in the [last Wikipedia table](https://en.wikipedia.org/wiki/List_of_stadiums_by_capacity#Capacity_of_40,000–50,000)...they should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good. Your final task is to save the table as a CSV file named `stadium_data.csv`. Remember how to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE TO SAVE THE STADIUM_DATA HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing a check to see that the file has been written to the current directory\n",
    "import os\n",
    "assert os.path.exists(\"stadium_data.csv\"), \"File named stadium_data.csv is not in the current directory.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
